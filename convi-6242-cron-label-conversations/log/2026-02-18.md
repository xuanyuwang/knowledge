# Project Log - 2026-02-18

## Progress

- Created project from Linear issue CONVI-6242
- Documented root cause: `cron-label-conversations` labels open conversations, never revisits after agent re-assignment or usecase reclassification
- Documented ClickHouse constraint: mutable columns in ORDER BY cause duplicate rows instead of replacements
- Cataloged three fix options (A: lookback window, B: filter by ended_at, C: change ORDER BY)
- Noted two in-flight PRs: go-servers#25706 (ended_at filter) and clickhouse-schema#172 (ORDER BY change)
- Outlined backfill plan for 2026 stale data after fix deployment
- Investigated cron code (`task.go`) and backfill-scorecards reference scripts
- Created detailed backfill-plan.md with deletion strategy, execution steps, and verification queries

## Details

### Key Insight: ClickHouse Dedup Problem

The `conversation_with_labels` table uses `ReplicatedReplacingMergeTree` with `agent_user_id` in the ORDER BY. When the agent changes, re-inserting the same conversation creates a new row rather than replacing the old one. This means any backfill must **delete before write**.

### Backfill Scope

- Customer: Alaska Air (at minimum, possibly others)
- Date range: 2026-01-01 to present (all 2026 data)
- Method: Re-run `cron-label-conversations` with delete-before-write over the target range

### Backfill Plan Investigation

Reviewed the cron code and `backfill-scorecards/` reference scripts. Key findings:

1. **Deletion is mandatory** — ORDER BY includes `agent_user_id` and `toStartOfHour(conversation_end_time)`, both mutable. Re-inserting creates duplicates. Even after schema migration (PR #172), old stale rows persist.

2. **Cron supports backfill natively** via env vars:
   - `LABEL_CONVERSATIONS_WITH_AGENT_ASSISTANCE_CONV_START_AT_RANGE_START` / `_END`
   - `FILTER_CUSTOMER_IN_LABEL_CONVERSATIONS_WITH_AGENT_ASSISTANCE`
   - When both time range vars are set, it skips the watermark query and uses the provided range directly.

3. **K8s job creation** follows the same pattern as backfill-scorecards: create job from `cron-sync-users` cronjob template → set env vars → apply.

4. **Deletion approach**: `ALTER TABLE ... DELETE WHERE customer_id = X AND conversation_end_time >= start AND conversation_end_time < end` on the local table with `ON CLUSTER`. Must wait for mutation to complete before re-inserting.

Full plan in `backfill-plan.md`.

### Scripts Created

Wrote four backfill scripts adapted from `backfill-scorecards/` patterns:

| Script | Purpose |
|--------|---------|
| `backfill.sh` | Single-run: create k8s job for one customer + date range |
| `delete_existing.sh` | ClickHouse DELETE via kubectl exec, with row count + confirmation |
| `backfill_sequential.py` | Full sequential: delete → wait mutation → create job → wait completion, per day, with JSON tracking and resume |
| `verify.sh` | Post-backfill verification: duplicates, row counts, Active Days, usecase distribution |

Key differences from backfill-scorecards:
- Cronjob template is `cron-label-conversations` (not `cron-batch-reindex-conversations`)
- No Temporal workflows — cron runs directly as k8s job, so we poll job status instead
- Delete step integrated into sequential script (runs per-day before each backfill)
- Customer filter is single-value env var (not comma-separated)

### Data Analysis

Queried both Postgres and ClickHouse for Alaska Air 2026 data:

- **Postgres** (with cron filters): 1,510,418 conversations
- **ClickHouse** (distributed): 1,636,467 rows / 1,633,276 distinct conversations
- **Existing duplicates**: 3,191 conversations with stale duplicate rows
- **3 shards**: local table holds ~1/3 of data; must query `_d` (distributed) for full counts
- **Database name**: `alaska_air_us_east_1` (per-customer databases, not `conversations`)

### Sample Backfill: Feb 18

Ran backfill for Feb 18 (~32K conversations) **without pre-deletion** to measure timing:

- **Job processing time: ~5 seconds** (from start to completion)
- Customer filter worked correctly: 88 profiles scanned, filtered to 1 (alaska-air)
- **73% of conversations got duplicated** (23,504 out of 32,144) — confirms deletion is mandatory
- 23,509 extra duplicate rows created (will be cleaned up during full backfill)

### Full Backfill Estimate

Best approach: bulk delete all 1.64M rows first, then run cron jobs day-by-day.
- DELETE mutation: ~10-30 minutes
- Cron jobs: ~5 seconds × 49 days = ~4 minutes
- **Total: ~15-35 minutes**

### Full Backfill Executed

Ran full backfill for Alaska Air at 01:32 UTC:

1. **Bulk DELETE**: 1,651,744 rows deleted in ~1 second, mutation instant
2. **49 cron jobs**: all completed in 15 min 17 sec, zero failures
3. **Total wall-clock time: ~16.5 minutes**

Results:
- 1,539,530 rows (was 1,651,744) — all clean, **0 duplicates**
- `reservations-chat` usecase dropped from ~405K to 307K (100K stale labels corrected)
- Rows match Postgres within ~2% (1.54M vs 1.51M)
