#!/usr/bin/env python3
"""
Check status of backfill jobs using Temporal CLI.

This script reads the tracking file generated by backfill_all.py and queries
Temporal for the status of each workflow.

Created: 2026-02-07
Updated: 2026-02-07

Prerequisites:
    - VPN connected
    - Port-forward to Temporal running:
      kubectl --context={cluster}_dev -n temporal port-forward svc/temporal-frontend-headless 7233:7233

Usage:
    python3 check_status.py --tracking backfill_tracking.json
    python3 check_status.py --tracking backfill_tracking.json --cluster us-east-1-prod
"""

import argparse
import json
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Optional


@dataclass
class WorkflowStatus:
    """Status of a Temporal workflow."""
    workflow_id: str
    status: str
    start_time: Optional[str] = None
    close_time: Optional[str] = None
    error: Optional[str] = None


def run_cmd(cmd: list[str], timeout: int = 30) -> tuple[int, str, str]:
    """Run a shell command and return (returncode, stdout, stderr)."""
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout
        )
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return -1, "", f"Command timed out after {timeout}s"
    except Exception as e:
        return -1, "", str(e)


def get_workflow_status(
    workflow_id: str,
    temporal_address: str = "localhost:7233",
    namespace: str = "ingestion"
) -> WorkflowStatus:
    """Query Temporal for workflow status."""
    cmd = [
        "temporal", "workflow", "describe",
        "--address", temporal_address,
        "--namespace", namespace,
        "--workflow-id", workflow_id,
        "--output", "json"
    ]

    rc, stdout, stderr = run_cmd(cmd)

    if rc != 0:
        return WorkflowStatus(
            workflow_id=workflow_id,
            status="unknown",
            error=stderr or "Failed to query workflow"
        )

    try:
        data = json.loads(stdout)
        execution_info = data.get("workflowExecutionInfo", {})
        status = execution_info.get("status", "UNKNOWN")
        start_time = execution_info.get("startTime")
        close_time = execution_info.get("closeTime")

        return WorkflowStatus(
            workflow_id=workflow_id,
            status=status,
            start_time=start_time,
            close_time=close_time
        )
    except json.JSONDecodeError as e:
        return WorkflowStatus(
            workflow_id=workflow_id,
            status="unknown",
            error=f"Failed to parse response: {e}"
        )


def list_workflows_by_prefix(
    prefix: str,
    temporal_address: str = "localhost:7233",
    namespace: str = "ingestion"
) -> list[dict]:
    """List workflows matching a prefix."""
    query = f'WorkflowId STARTS_WITH "{prefix}"'
    cmd = [
        "temporal", "workflow", "list",
        "--address", temporal_address,
        "--namespace", namespace,
        "--query", query,
        "--output", "json"
    ]

    rc, stdout, stderr = run_cmd(cmd, timeout=60)

    if rc != 0:
        print(f"Error listing workflows: {stderr}")
        return []

    try:
        return json.loads(stdout) if stdout else []
    except json.JSONDecodeError:
        return []


def load_tracking(tracking_path: str) -> dict:
    """Load tracking file."""
    with open(tracking_path) as f:
        return json.load(f)


def print_status_table(jobs: list[dict], statuses: dict[str, WorkflowStatus]):
    """Print a formatted status table."""
    print("\n" + "=" * 100)
    print(f"{'Customer':<20} {'Cluster':<18} {'Workflow ID':<45} {'Status':<12}")
    print("=" * 100)

    for job in jobs:
        customer = job.get("customer", "")
        cluster = job.get("cluster", "")
        workflow_id = job.get("temporal_workflow_id", "")

        if workflow_id and workflow_id in statuses:
            status = statuses[workflow_id].status
        elif workflow_id:
            status = "not-queried"
        else:
            status = "no-workflow-id"

        # Truncate workflow ID for display
        display_id = workflow_id[:42] + "..." if len(workflow_id) > 45 else workflow_id

        print(f"{customer:<20} {cluster:<18} {display_id:<45} {status:<12}")


def main():
    parser = argparse.ArgumentParser(
        description="Check status of backfill jobs using Temporal CLI"
    )
    parser.add_argument(
        "--tracking", "-t",
        required=True,
        help="Path to tracking JSON file from backfill_all.py"
    )
    parser.add_argument(
        "--temporal-address",
        default="localhost:7233",
        help="Temporal server address (default: localhost:7233)"
    )
    parser.add_argument(
        "--namespace",
        default="ingestion",
        help="Temporal namespace (default: ingestion)"
    )
    parser.add_argument(
        "--cluster",
        help="Only check jobs for a specific cluster"
    )
    parser.add_argument(
        "--customer",
        help="Only check jobs for a specific customer"
    )
    parser.add_argument(
        "--output", "-o",
        help="Output updated tracking to a new file"
    )

    args = parser.parse_args()

    # Load tracking data
    tracking = load_tracking(args.tracking)
    jobs = tracking.get("jobs", [])

    print("=" * 60)
    print("Backfill Status Checker")
    print("=" * 60)
    print(f"Tracking file: {args.tracking}")
    print(f"Total jobs: {len(jobs)}")
    print(f"Temporal address: {args.temporal_address}")

    # Filter jobs
    filtered_jobs = jobs
    if args.cluster:
        filtered_jobs = [j for j in filtered_jobs if j.get("cluster") == args.cluster]
    if args.customer:
        filtered_jobs = [j for j in filtered_jobs if j.get("customer") == args.customer]

    print(f"Checking: {len(filtered_jobs)} jobs")

    # Query status for each job
    statuses: dict[str, WorkflowStatus] = {}
    for job in filtered_jobs:
        workflow_id = job.get("temporal_workflow_id")
        if not workflow_id:
            continue

        print(f"\nQuerying: {workflow_id[:50]}...")
        status = get_workflow_status(
            workflow_id,
            args.temporal_address,
            args.namespace
        )
        statuses[workflow_id] = status

        if status.error:
            print(f"  Error: {status.error}")
        else:
            print(f"  Status: {status.status}")

    # Print summary table
    print_status_table(filtered_jobs, statuses)

    # Summary by status
    print("\n" + "=" * 60)
    print("Summary by Status")
    print("=" * 60)
    status_counts: dict[str, int] = {}
    for job in filtered_jobs:
        workflow_id = job.get("temporal_workflow_id")
        if workflow_id and workflow_id in statuses:
            s = statuses[workflow_id].status
        elif workflow_id:
            s = "not-queried"
        else:
            s = "no-workflow-id"
        status_counts[s] = status_counts.get(s, 0) + 1

    for status, count in sorted(status_counts.items()):
        print(f"  {status}: {count}")

    # Save updated tracking if requested
    if args.output:
        # Update job statuses in tracking
        for job in jobs:
            workflow_id = job.get("temporal_workflow_id")
            if workflow_id and workflow_id in statuses:
                job["temporal_status"] = statuses[workflow_id].status
                job["temporal_close_time"] = statuses[workflow_id].close_time

        output_data = {
            "generated_at": tracking.get("generated_at"),
            "status_checked_at": datetime.utcnow().isoformat() + "Z",
            "jobs": jobs
        }
        with open(args.output, 'w') as f:
            json.dump(output_data, f, indent=2)
        print(f"\nUpdated tracking saved to: {args.output}")


if __name__ == "__main__":
    main()
